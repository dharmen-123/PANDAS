Complete Pandas Syllabus: Beginner to Advanced Data Science
I. ðŸŒŸ Fundamentals and Core Structures (Beginner)
    1.0 Introduction and Setup
        1.0.1 Origin, Purpose, & Role:
            Pandas Origin (Wes McKinney) and Design Goals.
            Role in the Data Analysis Workflow (ETL/EDA).

        1.0.2 Installation & Setup:
            Installation (pip install pandas).
            Standard Import Convention (import pandas as pd).
            NumPy Dependency (Pandas is built on NumPy arrays).
            Pandas Documentation Overview.

    1.1 Series (1D Data Structure)
        1.1.1 Series Definition: Labeled, one-dimensional array.
        1.1.2 Creation Methods:
            From a List or NumPy Array.
            From a Dictionary (keys become the index).
            Empty Series Object.
            Creating a Series from a DataFrame Column.

        1.1.3 Series Attributes:
            .values (returns the data as a NumPy array).
            .index, .dtype, .size, .shape, .name.
            Handling the NaN Value (Missing Data).

        1.1.4 Accessing & Operations:
            Indexing and Slicing (by label and by position).
            Series Operations (Vectorization): Element-wise math operations (+, -, *, /).
            Basic Methods: .head(), .tail(), .sum(), .count(), .mean(), .nunique().

    1.2 DataFrame (2D Data Structure)
        1.2.1 DataFrame Definition: Labeled, two-dimensional table.

        1.2.2 Creation Methods:
            From a Dictionary of Lists/Series (most common).
            From a List of Dictionaries.
            From a NumPy array.
            From an Empty DataFrame.

        1.2.3 Structure & Attributes:
            Understanding Index (Row Labels) and Columns (Column Labels).
            Basic Attributes: .shape, .dtypes, .columns, .index, .values.
            Methods: .info(), .describe().

II. ðŸ’¾ Data I/O and Initial Inspection
    2.1 Input/Output Operations (I/O)
        2.1.1 Reading Data:
            pd.read_csv(): Arguments: sep, delimiter, header, index_col, na_values, dtype, parse_dates.
            pd.read_excel(): Arguments: sheet_name (handling multiple sheets).
            pd.read_json(), pd.read_html(), pd.read_clipboard().
            pd.read_sql(): Brief overview of database connectivity (requiring connection objects).

        2.1.2 Writing Data:
            df.to_csv(), 
            df.to_excel(), 
            df.to_json(), 
            df.to_sql().
            Controlling Index Output (index=False).

    2.2 Initial Data Inspection
        2.2.1 Viewing and Sampling Data:
            .head(n), .tail(n), .sample(n), .loc[] (basic inspection).

        2.2.2 Summarization and Structure:
            .info(): Data types and Non-Null Counts (Critical for missing data assessment).
            .describe(): Descriptive statistics for numerical data.
            .describe(include='all'): Including categorical data.

        2.2.3 Column Inspection:
            .value_counts(): Frequency analysis for categorical/discrete data.
            .unique(), .nunique().

III. ðŸŽ¯ Data Selection and Indexing (Intermediate)
    3.1 Basic Selection
        3.1.1 Column Selection: 
            Single column (returns Series) and Multiple columns (returns DataFrame) using basic bracket notation [].
        3.1.2 Row Slicing: 
            Selecting contiguous rows using integer positions df[start:end].
        3.1.3 Fancy Indexing: 
            Selecting non-contiguous rows/columns using a list of labels/positions.

    3.2 Label-based vs. Positional Indexing
        3.2.1 .loc[] (Label-based Indexing):
            Selecting by Index Labels and Column Names.
            The inclusive nature of slicing with .loc[].

        3.2.2 .iloc[] (Positional-based Indexing):
            Selecting by Integer Position (0-based) for both rows and columns.
            The exclusive nature of slicing with .iloc[] (Python standard).

        3.2.3 Mixed Indexing: 
            Using .ix[] (Deprecated) and the standard df.loc[:, 'ColName'] format.

    3.3 Conditional Selection (Boolean Filtering)
        3.3.1 Creating Boolean Series: 
            Conditions like df['col'] > 100.
        3.3.2 Filtering Rows: 
            Passing the Boolean Series to the DataFrame df[boolean_series].
        3.3.3 Multiple Conditions:
            Logical AND (&), OR (|), NOT (~).
            Using built-in methods: .isin(), .between().

IV. ðŸ§¹ Data Cleaning and Preparation
    4.1 Handling Missing Data (NaN)
        4.1.1 Identification: 
            df.isnull(), df.isna(), df.notnull().
        4.1.2 Dropping Missing Data:
            .dropna(): Arguments: axis (0 or 1), how ('any' or 'all'), thresh.

        4.1.3 Filling Missing Data (Imputation):
            .fillna(): Filling with a scalar value (e.g., 0, mean, median).
            Filling with Forward Fill (ffill/pad) and Backward Fill (bfill/backfill).
            .interpolate(): Interpolating missing values (e.g., linear, polynomial).

    4.2 Data Modification
        4.2.1 Adding and Deleting Columns: 
            Direct assignment, .insert(), .pop(), df.drop(columns=[...]).

        4.2.2 Renaming:
            Renaming Columns and Index Labels using .rename().
            Using the inplace=True parameter (understanding its impact).

        4.2.3 Data Type Conversion:
            .astype(): Converting to int, float, str, category.
            Specialized functions: pd.to_numeric(), pd.to_datetime().

    4.3 Handling Duplicates
        4.3.1 Identification: .duplicated(): Identifying duplicates (returns Boolean Series).
        4.3.2 Removal: .drop_duplicates(): Arguments: keep ('first', 'last', False), subset (columns to check).

V. ðŸ§® Aggregation and Grouping (Advanced)

    5.1 GroupBy Method
        5.1.1 The Split-Apply-Combine Strategy: 
            Understanding the process of grouping data.    
        5.1.2 Grouping by Criteria: 
            .groupby('ColName'), grouping by multiple columns.
        5.1.3 Iterating over Groups (for debugging/inspection).

    5.2 Aggregation Functions
        5.2.1 Basic Aggregations: 
            Applying .mean(), .sum(), .count(), .min(), .max(), .std(), .var() to grouped data.
        5.2.2 Applying Multiple Functions: 
            Using the .agg() method to compute different statistics on different columns simultaneously.
        5.2.3 Resetting the Index: 
            Using .reset_index() after aggregation.

    5.3 Function Application (Custom Logic)
        5.3.1 Series Application: 
            Using the .apply() method with custom functions or lambda expressions on a Series.
        5.3.2 DataFrame Application: 
            Using .apply() on rows (axis=1) or columns (axis=0).
        5.3.3 Transformation: 
            Using the .transform() method (returns a Series with the same index as the original DataFrame).
        5.3.4 .map() and .applymap() (briefly cover, noting .apply() is often preferred).

VI. ðŸ”— Data Combination

    6.1 Concatenation (pd.concat)
        6.1.1 Vertical Stacking: Stacking DataFrames by rows (axis=0).
        6.1.2 Horizontal Stacking: Stacking DataFrames by columns (axis=1).
        6.1.3 Handling Indices: Arguments: ignore_index, keys.

    6.2 Merging (pd.merge)
        6.2.1 SQL-style Joins: Understanding the how argument:
            Inner Join (how='inner', default).
            Left Join (how='left').
            Right Join (how='right').
            Outer Join (how='outer').
        6.2.2 Key Specification: 
            Arguments: on, left_on, right_on (merging on multiple keys).
        6.2.3 Index as Key: 
            Using left_index=True or right_index=True.

    6.3 Joining (df.join)
        6.3.1 Joining by Index Labels: 
            Simpler syntax primarily for joining DataFrames on their indices.
        6.3.2 Joining Multiple DataFrames (using a list of DataFrames in .join()).

VII. ðŸ”„ Data Reshaping and Pivoting
    7.1 Pivot Tables (pd.pivot_table)
        7.1.1 Definition: 
            Creating multi-dimensional summary tables (like Excel pivot tables).
        7.1.2 Core Arguments: 
            values, index, columns, aggfunc.
        7.1.3 Adding Margins (margins=True).

    7.2 Melt Function (pd.melt)
        7.2.1 Purpose: 
            Converting data from Wide to Long format (Normalization).
        7.2.2 Arguments: 
            id_vars (identifier columns) and value_vars (columns to melt).
    
    7.3 Hierarchical Indexing and Dimensional Conversion
        7.3.1 MultiIndex: 
            Creating and working with multiple index levels.

        7.3.2 .stack() and .unstack():
            .stack(): Moving the innermost column level to the innermost row index level (Wide to Long).
            .unstack(): Moving the innermost row index level to the innermost column level (Long to Wide).
            
        7.3.3 Simple Pivoting: Using df.pivot() (similar to pivot_table but without aggregation).

VIII. ðŸ”¬ Specialized Topics and Performance

    8.1 Time Series Functionality
        8.1.1 Datetime Objects:
            Converting strings to datetime using pd.to_datetime().
        8.1.2 .dt Accessor: 
            Extracting Date/Time components (.dt.year, .dt.dayofweek, .dt.month).
        8.1.3 Date Ranges and Offsets: 
            Creating a series of dates (pd.date_range()).
        8.1.4 Resampling:
            Changing the frequency of time series data (e.g., daily to monthly mean) using .resample().
        8.1.5 Rolling/Window Operations: 
            Calculating moving averages or sums using .rolling().

    8.2 Working with Text Data
        8.2.1 Vectorized String Operations: 
            Using the .str accessor for Series of strings.
            Case conversion (.str.lower(), .str.upper()).
            Checking strings (.str.contains(), .str.startswith()).

        8.2.2 Regular Expressions: 
            Using .str.match(), .str.extract(), .str.replace() with regex.
        8.2.3 Splitting and Joining: 
            .str.split().

    8.3 Performance and Advanced Data Types
        8.3.1 Categorical Data Type: 
            Using category dtype for memory efficiency and faster grouping on low-cardinality string colum ns.
        8.3.2 Performance Optimization: 
            Understanding Vectorization (the speed advantage of not using Python loops).
        8.3.3 Iteration: 
            Using .iterrows(), .itertuples() (and when to avoid them).
        8.3.4 Method Chaining: 
            Writing highly readable and efficient chained operations.
        8.3.5 Feature Extraction: 
            Applying custom business logic to create new features (a project concept using all manipulation skills).